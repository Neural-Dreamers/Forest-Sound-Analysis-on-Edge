{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3918,"status":"ok","timestamp":1698247234415,"user":{"displayName":"Thivindu Paranayapa","userId":"12191238824986368679"},"user_tz":-330},"id":"qS1qfLKQBmdF"},"outputs":[],"source":["import pandas as pd\n","import os\n","import numpy as np\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import gc\n","import pickle\n","import warnings\n","import tensorflow as tf\n","import optuna\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["Import dataset pickle file"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1698247238437,"user":{"displayName":"Thivindu Paranayapa","userId":"12191238824986368679"},"user_tz":-330},"id":"glshICHHBxB4"},"outputs":[],"source":["def get_pickle(pickle_dir, file_name):\n","    pickle_dir = os.path.join(os.getcwd(), 'datasets/fsc22/Pickle Files/' + pickle_dir)\n","    fold_dir = os.path.join(pickle_dir, file_name)\n","    infile = open(fold_dir, 'rb')\n","    fold = pickle.load(infile)\n","    infile.close()\n","\n","    return fold"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with tf.device('/CPU:0'):\n","    pickle_dir = input('Enter 5 fold pickle dirname: ')\n","    \n","    spect_folds = []\n","    train_spects = []\n","    \n","    for fold in range(5):\n","        spects_fold = get_pickle(pickle_dir, pickle_dir + '_fold' + str(fold+1))\n","        train_spects.extend(spects_fold)\n","        spect_folds.append(spects_fold)\n","\n","    print(f'len train_spects: {len(train_spects)}')\n","\n","    train_features_df = pd.DataFrame(train_spects, columns=['feature', 'class'])\n","\n","    del train_spects\n","\n","    gc.collect()\n","\n","    X_train = np.array(train_features_df['feature'].tolist())\n","    y_train = np.array(train_features_df['class'].tolist())\n","\n","    print(f'X_train shape: {np.shape(X_train)}')\n","    print(f'y_train shape: {np.shape(y_train)}')\n","    \n","    print(f'len spect_folds: {len(spect_folds)}') # num folds\n","    print(f'len spect_folds[0]: {len(spect_folds[0])}') # samples in a fold\n","    print(f'len spect_folds[0][0]: {len(spect_folds[0][0])}') # elements in a sample - should be 2\n","    print(f'shape spect_folds[0][0][0]: {np.shape(spect_folds[0][0][0])}') # spectrogram shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with tf.device('/CPU:0'):\n","    IMG_SIZE = (X_train.shape[1], X_train.shape[2])\n","    IMG_SHAPE = IMG_SIZE + (3,)\n","    print(IMG_SHAPE)\n","    input_shape = IMG_SHAPE\n","\n","    num_labels = 26 # Should be changed"]},{"cell_type":"markdown","metadata":{},"source":["Create Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3483,"status":"ok","timestamp":1698074876087,"user":{"displayName":"Meelan Bandra","userId":"02656530806929428399"},"user_tz":-330},"id":"vNS_YkL82OLA","outputId":"f66ee970-e411-4611-aa74-dbcf40076ef3"},"outputs":[],"source":["def get_model():\n","    base_model = tf.keras.applications.MobileNetV3Small(\n","        input_shape=IMG_SHAPE,\n","        weights=None,\n","        include_top=False\n","    )\n","    \n","    x = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')(base_model.output)\n","    outputs = tf.keras.layers.Dense(num_labels, activation='softmax', name='prediction')(x)\n","\n","    model = tf.keras.models.Model(base_model.input, outputs, name='MobileNetV3-Small_FSC22')\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["Draw accuracy loss plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_plot_accuracy_loss(history, model_name, model_save_dir, f=1):\n","    epochs = history.epoch\n","    tr_acc = history.history['accuracy']\n","    tr_loss = history.history['loss']\n","    val_acc = history.history['val_accuracy']\n","    val_loss = history.history['val_loss']\n","    run_epochs = len(epochs)\n","\n","    f = f\n","    save_epochs = [epochs[i] for i in range(0, run_epochs, f)]\n","    save_tr_acc = [tr_acc[i] for i in range(0, run_epochs, f)]\n","    save_tr_loss = [tr_loss[i] for i in range(0, run_epochs, f)]\n","    save_val_acc = [val_acc[i] for i in range(0, run_epochs, f)]\n","    save_val_loss = [val_loss[i] for i in range(0, run_epochs, f)]\n","\n","    # Create a figure and axis\n","    fig, ax1 = plt.subplots()\n","\n","    fig.set_figheight(12)\n","    fig.set_figwidth(24)\n","\n","    # Plot accuracy lines\n","    ax1.set_xlabel('Epochs')\n","    ax1.set_ylabel('Accuracy', color='black')\n","    ax1.plot(save_epochs, save_tr_acc, color='#800000', marker='o', label='Training Accuracy')\n","    ax1.plot(save_epochs, save_val_acc, color='#000075', marker='x', label='Validation Accuracy')\n","    # ax1.set_xticklabels(save_epochs, rotation=90)\n","\n","    # Create a second y-axis for loss lines\n","    ax2 = ax1.twinx()  # Share the same x-axis\n","    ax2.set_ylabel('Loss', color='black')\n","    ax2.plot(save_epochs, save_tr_loss, color='#3cb44b', marker='s', label='Training Loss')\n","    ax2.plot(save_epochs, save_val_loss, color='#f58231', marker='^', label='Validation Loss')\n","    ax2.tick_params(axis='y', labelcolor='black')\n","\n","    # Add a legend\n","    lines, labels = ax1.get_legend_handles_labels()\n","    lines2, labels2 = ax2.get_legend_handles_labels()\n","    ax2.legend(lines + lines2, labels + labels2, loc='best')\n","\n","    # Set a title\n","    plt.title('Accuracy and Loss Over Epochs')\n","\n","    accuracy_matrices_path = model_save_dir\n","\n","    curr_datetime = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n","    filename = f'{model_name.lower()}-training_metrics_plot-{format(curr_datetime)}.png'\n","\n","    if not os.path.exists(accuracy_matrices_path):\n","        os.makedirs(accuracy_matrices_path)\n","\n","    # Save the plot to the specified folder\n","    destination = os.path.join(accuracy_matrices_path, filename)\n","    plt.savefig(destination, bbox_inches='tight')"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter tuning"]},{"cell_type":"markdown","metadata":{},"source":["Define search space"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train_hp, X_test_hp, y_train_hp, y_test_hp = train_test_split(X_train, y_train, test_size=0.3, random_state=42, shuffle=True, stratify=y_train)\n","\n","def objective(trial):\n","    # Sample hyperparameters from the search space\n","    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-1)\n","    optimizer = trial.suggest_categorical(\"optimizer\", [\"adam\", \"sgd\"])\n","    num_epochs = trial.suggest_int(\"num_epochs\", 20, 75)\n","    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n","\n","    opt_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate) if optimizer=='adam' else tf.keras.optimizers.SGD(learning_rate=learning_rate)\n","\n","    model = get_model()\n","\n","    # Compile the model\n","    model.compile(optimizer=opt_optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","    earlystopping = tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\", mode=\"max\", patience=10, restore_best_weights=True, verbose=1)\n","\n","    # Train the model with the given hyperparameters\n","    model.fit(X_train_hp, y_train_hp, epochs=num_epochs, batch_size=batch_size, callbacks =[earlystopping])\n","\n","    # Evaluate the model on the test set\n","    accuracy = model.evaluate(X_test_hp, y_test_hp)[1]\n","\n","    del model\n","\n","    gc.collect()\n","\n","    return accuracy"]},{"cell_type":"markdown","metadata":{},"source":["Run hyperparameter tuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a study object\n","study = optuna.study.create_study(direction=\"maximize\")\n","\n","# Run the optimization\n","study.optimize(objective, n_trials=50)\n","\n","del X_train_hp\n","del X_test_hp\n","del y_train_hp\n","del y_test_hp\n","\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the hyperparameters\n","hyperparameter_optimization_path = os.path.join(os.getcwd(), './metrics/hyperparameter_optimization')\n","\n","curr_datetime = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n","filename = f'mobilenetv3s-hyperparameter_optimization_plot-{format(curr_datetime)}.png'\n","\n","if not os.path.exists(hyperparameter_optimization_path):\n","    os.makedirs(hyperparameter_optimization_path)\n","\n","# Save the plot to the specified folder\n","destination = os.path.join(hyperparameter_optimization_path, filename)\n","\n","optuna.visualization.matplotlib.plot_optimization_history(study)\n","plt.savefig(destination, bbox_inches='tight')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# best hyperparameters found by our experiments\n","\n","best_learning_rate = 0.06325014821\n","best_optimizer = 'sgd'\n","best_num_epochs = 70\n","best_batch_size = 16"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the best hyperparameters\n","best_trial = study.best_trial\n","\n","# Get the values of the best hyperparameters\n","best_learning_rate = best_trial.params[\"learning_rate\"]\n","best_optimizer = best_trial.params[\"optimizer\"] \n","best_num_epochs = best_trial.params[\"num_epochs\"]\n","best_batch_size = best_trial.params[\"batch_size\"]\n","\n","print(f'best_learning_rate: {best_learning_rate}')\n","print(f'best_optimizer: {best_optimizer}')\n","print(f'best_num_epochs: {best_num_epochs}')\n","print(f'best_batch_size: {best_batch_size}')"]},{"cell_type":"markdown","metadata":{},"source":["### Train model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with tf.device('/CPU:0'):\n","    model_name = input('Enter model name: ')\n","\n","    models_dir = os.path.join(os.getcwd(), 'models')\n","    model_dir = os.path.join(models_dir, \"MobileNetV3-Small\")\n","    model_save_dir = os.path.join(model_dir, model_name)\n","    model_save_path = os.path.join(model_save_dir, model_name + \"_base.h5\")\n","\n","\n","    if not os.path.exists(model_save_dir):\n","        os.makedirs(model_save_dir)\n","\n","    model = get_model()\n","    model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_train_vaild_data(spects, fold):\n","    train_spects = []\n","    valid_spects = None\n","    \n","    for i in range(5):\n","        if i + 1 != fold:\n","            train_spects.extend(spects[i])\n","        else:\n","            valid_spects = spects[i]\n","    \n","    train_df = pd.DataFrame(train_spects, columns=['feature', 'class'])\n","    valid_df = pd.DataFrame(valid_spects, columns=['feature', 'class'])\n","\n","    del train_spects\n","    del valid_spects\n","\n","    gc.collect()\n","\n","    X_train_cv = np.array(train_df['feature'].tolist())\n","    y_train_cv = np.array(train_df['class'].tolist())\n","    \n","    X_valid_cv = np.array(valid_df['feature'].tolist())\n","    y_valid_cv = np.array(valid_df['class'].tolist())\n","    \n","    return X_train_cv, X_valid_cv, y_train_cv, y_valid_cv\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["acc_per_fold = []\n","loss_per_fold = []\n","\n","# K-fold Cross Validation model evaluation\n","fold_no = 1\n","\n","for fold in range(5):\n","    base_learning_rate = best_learning_rate\n","\n","    initial_epochs = best_num_epochs\n","    batch_size = best_batch_size\n","\n","    final_learning_rate = 0.0005\n","    learning_rate_decay_factor = (final_learning_rate / base_learning_rate)**(1/initial_epochs)\n","    \n","    X_train_cv, X_valid_cv, y_train_cv, y_valid_cv = get_train_vaild_data(spect_folds, fold_no)\n","    \n","    # print(f'X_train_cv shape: {np.shape(X_train_cv)}')\n","    # print(f'y_train_cv shape: {np.shape(y_train_cv)}')\n","\n","    # print(f'X_valid_cv shape: {np.shape(X_valid_cv)}')\n","    # print(f'y_valid_cv shape: {np.shape(y_valid_cv)}')\n","    \n","    steps_per_epoch = int(np.shape(X_train_cv)[0]/batch_size)\n","\n","    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","                    initial_learning_rate=base_learning_rate,\n","                    decay_steps=steps_per_epoch,\n","                    decay_rate=learning_rate_decay_factor,\n","                    staircase=True)\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule) if best_optimizer=='adam' else tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n","\n","    model = get_model()\n","    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","    # Generate a print\n","    print('------------------------------------------------------------------------')\n","    print(f'Training for fold {fold_no} ...')\n","\n","    earlystopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=10, restore_best_weights=True, verbose=1)\n","\n","    # Fit data to model\n","    history = model.fit(X_train_cv, y_train_cv,\n","                batch_size=batch_size,\n","                epochs=initial_epochs,\n","                validation_data=(X_valid_cv, y_valid_cv),\n","                verbose=1, callbacks =[earlystopping])\n","\n","    # Generate generalization metrics\n","    scores = model.evaluate(X_valid_cv, y_valid_cv, verbose=0)\n","    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n","    acc_per_fold.append(scores[1] * 100)\n","    loss_per_fold.append(scores[0])\n","\n","    model_split_save_path = os.path.join(model_save_dir, model_name + \"_base_fold\" + str(fold_no) + \".h5\")\n","    \n","    # Increase fold number\n","    fold_no = fold_no + 1\n","    \n","    model.save(model_split_save_path, include_optimizer=False)\n","\n","    del model\n","\n","    gc.collect()\n","\n","# == Provide average scores ==\n","print('------------------------------------------------------------------------')\n","print('Score per fold')\n","\n","for i in range(0, len(acc_per_fold)):\n","    print('------------------------------------------------------------------------')\n","    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n","\n","print('------------------------------------------------------------------------')\n","print('Average scores for all folds:')\n","print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n","print(f'> Loss: {np.mean(loss_per_fold)}')\n","print('------------------------------------------------------------------------')\n","\n","save_plot_accuracy_loss(history, model_name, model_save_dir, 1)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1HO9cXPyiw2XriBpyeSCDC60cVKSMHH6-","timestamp":1698240786400},{"file_id":"1jSqbmXFc3WqBUu7uHhwUk_RytnIleTMA","timestamp":1693504342683}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
